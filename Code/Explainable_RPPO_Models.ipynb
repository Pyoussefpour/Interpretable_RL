{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable Recurrent PPO: Determining Proper Insulin Dosage\n",
        "\n",
        "Parsa Youssefpour, Bo Gong, Pak Hop Chan"
      ],
      "metadata": {
        "id": "Mn9OmFXCsa16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required installations"
      ],
      "metadata": {
        "id": "Tj2ccu5CtM4z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqkCT8basULu"
      },
      "outputs": [],
      "source": [
        "!pip install stable_baselines3\n",
        "!pip install sb3_contrib\n",
        "!pip install simglucose"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Enviroment"
      ],
      "metadata": {
        "id": "vvWWTyJytqqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from simglucose.simulation.scenario_gen import RandomScenario\n",
        "from datetime import datetime\n",
        "from simglucose.simulation.scenario import CustomScenario\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "now = datetime.now()\n",
        "start_time = datetime.combine(now.date(), datetime.min.time())\n",
        "\n",
        "patient_name = [\n",
        "    \"adult#001\",\n",
        "    \"adult#002\",\n",
        "    \"adult#003\",\n",
        "    \"adult#004\",\n",
        "    \"adult#005\",\n",
        "]\n",
        "\n",
        "# Randomized Meal Plan:\n",
        "scenario = RandomScenario(start_time=start_time, seed=1)\n",
        "\n",
        "# Custom meal Plan:\n",
        "# scen = [(7, 45), (12, 70), (16, 15), (18, 80), (23, 10)]\n",
        "# scenario = CustomScenario(start_time=start_time, scenario=scen)\n",
        "\n",
        "register(\n",
        "    id=\"simglucose_attn\",\n",
        "    entry_point=\"simglucose.envs:T1DSimGymnaisumEnv\",\n",
        "    max_episode_steps=480,  # 24 hours at 3-min steps\n",
        "    kwargs={\"patient_name\": patient_name,\n",
        "            \"custom_scenario\": scenario},\n",
        ")"
      ],
      "metadata": {
        "id": "05MWd52jsaPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium\n",
        "from gymnasium.spaces import Box as GymnasiumBox\n",
        "import numpy as np\n",
        "\n",
        "# Custom wrapper to modify observation and the action space\n",
        "class CustomSimglucoseWrapper(gymnasium.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        low = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float32)  # [CGM, last_insulin, meal, hour]\n",
        "        high = np.array([2, 1, 4, 23.0/23], dtype=np.float32)\n",
        "        self.observation_space = GymnasiumBox(low, high)\n",
        "\n",
        "\n",
        "        self.action_space = GymnasiumBox(\n",
        "            low=np.array([0.0], dtype=np.float32),\n",
        "            high=np.array([1], dtype=np.float32)\n",
        "        )\n",
        "\n",
        "        self.last_insulin = 0.0\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        cgm,info = self.env.reset(**kwargs)\n",
        "        meal = info['meal']\n",
        "        hour = info['time'].hour\n",
        "        self.last_insulin = 0.0\n",
        "        obs = obs = np.array([cgm[0]/300, self.last_insulin/0.1, meal/50, hour/23], dtype=np.float32) #Features Scaled to what worked best in training (not normalized)\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        action = float(np.clip(action[0], 0.0, 1.0))\n",
        "        self.last_insulin = action\n",
        "\n",
        "        cgm, reward, terminated, truncated, info = self.env.step(scaled_action)\n",
        "\n",
        "        bg = info['bg']\n",
        "        meal = info['meal']\n",
        "        hour = info['time'].hour\n",
        "        obs = np.array([cgm[0]/300, self.last_insulin/0.1, meal/50, hour/23], dtype=np.float32)\n",
        "\n",
        "        if bg < 54 or bg > 300:\n",
        "          terminated = True\n",
        "\n",
        "        return obs, reward, bool(terminated), bool(truncated), info"
      ],
      "metadata": {
        "id": "PIPBJ7TjuVgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model (PPO) - Training"
      ],
      "metadata": {
        "id": "fGy0gKf3vKMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import os\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from google.colab import files\n",
        "\n",
        "#load the Environment\n",
        "env = CustomSimglucoseWrapper(gymnasium.make(\"simglucose_attn\"))\n",
        "env = Monitor(env, filename=\"baseline_PPO\")                                     # data for the training reward curve\n",
        "\n",
        "#Eval_callback to save the best model\n",
        "eval_callback = EvalCallback(\n",
        "    env,\n",
        "    best_model_save_path=\"./best_model_PPO2/\",\n",
        "    log_path=\"./logs/\",\n",
        "    eval_freq=5000,\n",
        "    n_eval_episodes=10,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "#Model Architecture - Separate layers for actor (pi) and critic (vf)\n",
        "policy_kwargs = dict(\n",
        "    net_arch=dict(pi=[256, 64, 64], vf=[256, 64, 64]),\n",
        ")\n",
        "\n",
        "model = PPO(\n",
        "    policy=\"MlpPolicy\",\n",
        "    env=env,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    verbose=1,\n",
        "    gamma=0.99,\n",
        "    n_steps=512,\n",
        "    batch_size=64,\n",
        "    learning_rate=3e-4,\n",
        "    ent_coef=0.3,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500_000, progress_bar= True, callback= eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"baseline_PPO\")"
      ],
      "metadata": {
        "id": "jiVMHd4vwRAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent PPO Model"
      ],
      "metadata": {
        "id": "b6ah7iVNzM2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sb3_contrib import RecurrentPPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import os\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "# load the enviroment\n",
        "env = CustomSimglucoseWrapper(gymnasium.make(\"simglucose_attn\"))\n",
        "env = Monitor(env, filename=\"Recurrent_PPO.csv\")                                # data for the training reward curve\n",
        "\n",
        "# Call back to save the best model\n",
        "eval_callback = EvalCallback(\n",
        "    env,\n",
        "    best_model_save_path=\"./best_model_og/\",\n",
        "    log_path=\"./logs/\",\n",
        "    eval_freq=5000,\n",
        "    n_eval_episodes=10,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Model Architecture\n",
        "model = RecurrentPPO(\n",
        "    policy=\"MlpLstmPolicy\",\n",
        "    env=env,\n",
        "    n_steps=512,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    learning_rate=5e-5,\n",
        "    ent_coef=0.3,\n",
        "    verbose=1,\n",
        "    seed=1,\n",
        "    device=\"cuda\",\n",
        "    clip_range=0.1,\n",
        "    max_grad_norm= 0.5\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500_000, progress_bar= True, callback = eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"RPPO\")"
      ],
      "metadata": {
        "id": "B72dt9TyyXZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Recurrent PPO"
      ],
      "metadata": {
        "id": "vmBpqtRr2ZIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Optional, Union, Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common.distributions import Distribution\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.torch_layers import (\n",
        "    BaseFeaturesExtractor,\n",
        "    FlattenExtractor,\n",
        "    MlpExtractor,\n",
        ")\n",
        "from stable_baselines3.common.type_aliases import Schedule\n",
        "from stable_baselines3.common.utils import zip_strict\n",
        "from torch import nn\n",
        "\n",
        "from sb3_contrib.common.recurrent.type_aliases import RNNStates\n",
        "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
        "\n",
        "class FeatureAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Get attention and attention features\n",
        "    dynamic attention weights for each observation.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dim: int):\n",
        "        super().__init__()\n",
        "        # Dense layer to calculate attention weights\n",
        "        self.attention_layer = nn.Sequential(\n",
        "            nn.Linear(feature_dim, feature_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feature_dim // 2, feature_dim),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Store last calculated weights for explainability\n",
        "        self.last_attention_weights = None\n",
        "\n",
        "    def forward(self, x: th.Tensor) -> th.Tensor:\n",
        "        \"\"\"\n",
        "        Apply observation-specific feature attention.\n",
        "\n",
        "        inputs:\n",
        "            x: Input tensor with shape (batch_size, feature_dim)\n",
        "\n",
        "        Returns:\n",
        "            Weighted features with shape (batch_size, feature_dim)\n",
        "        \"\"\"\n",
        "        # Calculate attention weights for each observation in batch\n",
        "        attention_weights = self.attention_layer(x)\n",
        "\n",
        "        # print(\"attn weights:\", attention_weights)\n",
        "        # Store for later retrieval (saving last batch)\n",
        "        self.last_attention_weights = attention_weights\n",
        "\n",
        "        # Apply attention weights to features\n",
        "        weighted_features = x * attention_weights\n",
        "\n",
        "        return weighted_features\n",
        "\n",
        "    def get_attention_weights(self, batch_idx: int = 0) -> th.Tensor:\n",
        "        \"\"\"\n",
        "        Get the attention weights for a specific observation in the last batch.\n",
        "\n",
        "        Input:\n",
        "            batch_idx: Index of the observation in the batch\n",
        "\n",
        "        Returns:\n",
        "            Attention weights tensor\n",
        "        \"\"\"\n",
        "        if self.last_attention_weights is None:\n",
        "            raise ValueError(\"No attention weights calculated yet. Run a forward pass first.\")\n",
        "\n",
        "        # Return weights for the specified observation\n",
        "        return self.last_attention_weights[batch_idx]\n",
        "\n",
        "\n",
        "class AttentionFeaturesExtractor(BaseFeaturesExtractor):\n",
        "    \"\"\"\n",
        "    Create a richer representation vector to be sent to the Recurrent PPO.\n",
        "    Changes the type to BaseFeaturesExtractor to be applied in the AttentionLstmPPOPolicy for sb3_contrib Recurrent PPO.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Space,\n",
        "        features_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(observation_space, features_dim)\n",
        "\n",
        "        n_input_features = int(np.prod(observation_space.shape))\n",
        "\n",
        "        # Attention layer for input features\n",
        "        self.attention = FeatureAttention(n_input_features)\n",
        "\n",
        "        # Feature processing layers (after attention)\n",
        "        self.feature_layers = nn.Sequential(\n",
        "            nn.Linear(n_input_features, features_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(features_dim, features_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        # Apply attention to raw features first\n",
        "        attended_features = self.attention(observations)\n",
        "\n",
        "        # Then process through the feature layers\n",
        "        features = self.feature_layers(attended_features)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "# taken and modified from: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/recurrent/policies.py\n",
        "# Added the attention retrieval\n",
        "\n",
        "class AttentionLstmPPOPolicy(RecurrentActorCriticPolicy):\n",
        "    \"\"\"\n",
        "    Recurrent policy class for actor-critic algorithms with attention mechanism for explainability.\n",
        "    To be used with RecurrentPPO.\n",
        "\n",
        "    :param observation_space: Observation space\n",
        "    :param action_space: Action space\n",
        "    :param lr_schedule: Learning rate schedule (could be constant)\n",
        "    :param net_arch: The specification of the policy and value networks.\n",
        "    :param activation_fn: Activation function\n",
        "    :param ortho_init: Whether to use or not orthogonal initialization\n",
        "    :param use_sde: Whether to use State Dependent Exploration or not\n",
        "    :param log_std_init: Initial value for the log standard deviation\n",
        "    :param full_std: Whether to use (n_features x n_actions) parameters\n",
        "        for the std instead of only (n_features,) when using gSDE\n",
        "    :param use_expln: Use ``expln()`` function instead of ``exp()`` to ensure\n",
        "        a positive standard deviation (cf paper). It allows to keep variance\n",
        "        above zero and prevent it from growing too fast. In practice, ``exp()`` is usually enough.\n",
        "    :param squash_output: Whether to squash the output using a tanh function,\n",
        "        this allows to ensure boundaries when using gSDE.\n",
        "    :param features_extractor_class: Features extractor to use.\n",
        "    :param features_extractor_kwargs: Keyword arguments\n",
        "        to pass to the features extractor.\n",
        "    :param share_features_extractor: If True, the features extractor is shared between the policy and value networks.\n",
        "    :param normalize_images: Whether to normalize images or not,\n",
        "         dividing by 255.0 (True by default)\n",
        "    :param optimizer_class: The optimizer to use,\n",
        "        ``th.optim.Adam`` by default\n",
        "    :param optimizer_kwargs: Additional keyword arguments,\n",
        "        excluding the learning rate, to pass to the optimizer\n",
        "    :param lstm_hidden_size: Number of hidden units for each LSTM layer.\n",
        "    :param n_lstm_layers: Number of LSTM layers.\n",
        "    :param shared_lstm: Whether the LSTM is shared between the actor and the critic\n",
        "        (in that case, only the actor gradient is used)\n",
        "        By default, the actor and the critic have two separate LSTM.\n",
        "    :param enable_critic_lstm: Use a seperate LSTM for the critic.\n",
        "    :param lstm_kwargs: Additional keyword arguments to pass the the LSTM\n",
        "        constructor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Space,\n",
        "        action_space: spaces.Space,\n",
        "        lr_schedule: Schedule,\n",
        "        net_arch: Optional[Union[list[int], dict[str, list[int]]]] = None,\n",
        "        activation_fn: type[nn.Module] = nn.Tanh,\n",
        "        ortho_init: bool = True,\n",
        "        use_sde: bool = False,\n",
        "        log_std_init: float = 0.0,\n",
        "        full_std: bool = True,\n",
        "        use_expln: bool = False,\n",
        "        squash_output: bool = False,\n",
        "        features_extractor_class: type[BaseFeaturesExtractor] = AttentionFeaturesExtractor,\n",
        "        features_extractor_kwargs: Optional[dict[str, Any]] = None,\n",
        "        share_features_extractor: bool = True,\n",
        "        normalize_images: bool = True,\n",
        "        optimizer_class: type[th.optim.Optimizer] = th.optim.Adam,\n",
        "        optimizer_kwargs: Optional[dict[str, Any]] = None,\n",
        "        lstm_hidden_size: int = 256,\n",
        "        n_lstm_layers: int = 1,\n",
        "        shared_lstm: bool = False,\n",
        "        enable_critic_lstm: bool = True,\n",
        "        lstm_kwargs: Optional[dict[str, Any]] = None,\n",
        "    ):\n",
        "        # Initialize with parent class\n",
        "        super().__init__(\n",
        "            observation_space,\n",
        "            action_space,\n",
        "            lr_schedule,\n",
        "            net_arch,\n",
        "            activation_fn,\n",
        "            ortho_init,\n",
        "            use_sde,\n",
        "            log_std_init,\n",
        "            full_std,\n",
        "            use_expln,\n",
        "            squash_output,\n",
        "            features_extractor_class,\n",
        "            features_extractor_kwargs,\n",
        "            share_features_extractor,\n",
        "            normalize_images,\n",
        "            optimizer_class,\n",
        "            optimizer_kwargs,\n",
        "            lstm_hidden_size,\n",
        "            n_lstm_layers,\n",
        "            shared_lstm,\n",
        "            enable_critic_lstm,\n",
        "            lstm_kwargs,\n",
        "        )\n",
        "\n",
        "        # Access the attention layer from the features extractor\n",
        "        if hasattr(self.features_extractor, 'attention'):\n",
        "            self.attention = self.features_extractor.attention\n",
        "        else:\n",
        "            # Fallback if the features extractor was overridden (just in case)\n",
        "            self.attention = None\n",
        "\n",
        "    @property\n",
        "    def feature_names(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Return the feature names for explainability.\n",
        "        This should be customized based on your environment.\n",
        "        used @property decorator to turn into read only attribute. Ensuring no accidental changes/overwriting.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return [\"CGM\", \"Last_Insulin\", \"Meal\", \"Hour\"]\n",
        "\n",
        "    def get_attention_weights(self) -> th.Tensor:\n",
        "        \"\"\"\n",
        "        Get the attention weights from the feature attention.\n",
        "        \"\"\"\n",
        "        if self.attention is None:\n",
        "            raise ValueError(\"Attention mechanism not available in this policy.\")\n",
        "        return self.attention.get_attention_weights()\n",
        "\n",
        "    def explain(self, obs: np.ndarray) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Return feature importance as a dictionary.\n",
        "\n",
        "        Inputs:\n",
        "            obs: Observation (state)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping feature names to their importance scores\n",
        "        \"\"\"\n",
        "        # Convert observation to tensor\n",
        "        device = self.device\n",
        "        obs_tensor = th.as_tensor(obs).float().to(device)\n",
        "\n",
        "        # Ensure proper shape\n",
        "        if len(obs_tensor.shape) == 1:\n",
        "            obs_tensor = obs_tensor.unsqueeze(0)\n",
        "\n",
        "        # Just extract features to trigger attention computation\n",
        "        with th.no_grad():\n",
        "            # Use the extract_features method directly without requiring LSTM states\n",
        "            _ = self.features_extractor(obs_tensor)\n",
        "\n",
        "        # Get attention weights\n",
        "        if not hasattr(self, 'attention') or self.attention is None:\n",
        "            raise ValueError(\"This policy doesn't have an attention mechanism\")\n",
        "\n",
        "        attention_weights = self.attention.get_attention_weights().detach().cpu().numpy()\n",
        "        # print(\"attn Weights:\", attention_weights)\n",
        "\n",
        "        # Matching weights with feature names\n",
        "        feature_names = self.feature_names\n",
        "\n",
        "        # Handling potental dimension mismatch\n",
        "        if len(feature_names) !=len(attention_weights):\n",
        "            print(f\"Warning: feature_names length ({len(feature_names)}) doesn't match \"\n",
        "                  f\"attention_weights length ({len(attention_weights)})\")\n",
        "            # Truncate the longer one\n",
        "            min_len = min(len(feature_names), len(attention_weights))\n",
        "            feature_names = feature_names[:min_len]\n",
        "            attention_weights = attention_weights[:min_len]\n",
        "\n",
        "        feature_importance = dict(zip(feature_names, attention_weights.flatten()))\n",
        "\n",
        "        return feature_importance\n",
        "\n",
        "\n",
        "# Modified R-PPO architecture kwargs\n",
        "def create_attention_lstm_policy():\n",
        "    policy_kwargs = dict(\n",
        "        features_extractor_class=AttentionFeaturesExtractor,\n",
        "        features_extractor_kwargs=dict(features_dim=64),\n",
        "        net_arch=dict(\n",
        "            pi=[64, 64],\n",
        "            vf=[64, 64]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return AttentionLstmPPOPolicy, policy_kwargs"
      ],
      "metadata": {
        "id": "4-bnkYqE2dqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "Xk3cAjJ22uFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decaying Entropy Coefficient - Tested but NOT USED in Final model\n",
        "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
        "\n",
        "class EntropyDecayCallback(BaseCallback):\n",
        "    def __init__(self, initial_value=0.5, final_value=0.01, max_timesteps=1_000_000, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.initial_value = initial_value\n",
        "        self.final_value = final_value\n",
        "        self.max_timesteps = max_timesteps\n",
        "\n",
        "    def _on_step(self):\n",
        "        progress = min(1.0, self.num_timesteps / self.max_timesteps)\n",
        "        new_ent_coef = self.initial_value * (1 - progress) + self.final_value * progress\n",
        "        self.model.ent_coef = new_ent_coef\n",
        "        return True"
      ],
      "metadata": {
        "id": "uuxtX5112tcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sb3_contrib import RecurrentPPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from google.colab import files\n",
        "\n",
        "# Create your environment\n",
        "env = CustomSimglucoseWrapper(gymnasium.make(\"simglucose_attn\"))\n",
        "env = Monitor(env, filename=\"Attention_RPPO\")\n",
        "\n",
        "\n",
        "# Call back to save the best model\n",
        "eval_callback = EvalCallback(\n",
        "    env,\n",
        "    best_model_save_path=\"./best_model_A_RPPO/\",\n",
        "    log_path=\"./logs/\",\n",
        "    eval_freq=5000,\n",
        "    n_eval_episodes=10,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Get the custom policy and kwargs\n",
        "policy_class, policy_kwargs = create_attention_lstm_policy()\n",
        "\n",
        "\n",
        "# Create the model with custom policy\n",
        "model = RecurrentPPO(\n",
        "    policy=policy_class,\n",
        "    env=env,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    n_steps=512,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    learning_rate=5e-5,\n",
        "    ent_coef=0.3,\n",
        "    verbose=1,\n",
        "    seed=1,\n",
        "    device=\"cuda\",\n",
        "    clip_range=0.1,\n",
        "    max_grad_norm= 0.5\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500_000, progress_bar= True, callback= eval_callback)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"Attention_RPPO\")\n",
        "\n",
        "# Download the model file\n",
        "files.download('Attention_RPPO.zip')"
      ],
      "metadata": {
        "id": "6Ls6Bzns3V04"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}